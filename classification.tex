\chapter{Classification}

\vspace{5 mm}
\noindent
For a classification problem, we are given a dataset of size $n$ where each 
data point falls into one of $c$ classes. Each data point contains $p$ features. 
For simplicity, we will assume that each feature is a continuous real number. 
We believe that each class $c$ will appear in clusters. That is, we think that 
the differences between the feature values within the class are smaller than 
the differences out of class. In addition, we have an additional dataset of 
size $m$ of which we do not know their classes. We would like to predict these 
data points classes using our dataset of size $n$ (our training data). Given 
this information, and assuming that our feature dimension $p$ is sufficiently 
small or we have data size $n >> p$ such that we can overcome the curse of 
dimensionality associated with distance problems, we would like to apply the 
K nearest neighbors classification scheme.
\textbf{ADD CITATION}

\vspace{5 mm}
\noindent
The general idea is we will compare each data point in our classification data 
set to all of the data in our training data. We compute the distance between 
the data we wish to classify and all of the training data. The distance measure 
can be a variety of metrics (e.g. the Euclidean distance, absolute distance, 
some user defined distance metric, etc...). For our purposes, we will use the 
Euclidean distance metric for distances. The $K$ training data points with the 
smallest computed distance are the data points we will classify on. We classify 
the point to one of the $c$ classes that appears the most in the $K$ selected 
training points.

\vspace{5 mm}
\noindent
There are generally two methods for computing K nearest neighbors.

\vspace{5 mm}
\noindent
\textbf{MAKE SECTION HEADER}

You can iterate through each point you would wish to classify, compute its 
distance from every training data point, select the closest $K$ points, and 
return the class that appears the most of those $K$ points. The psuedo code for 
this process is as follows:

\begin{algorithm}
%-------------------------------Header------------------------------------------
\DontPrintSemicolon
\KwData{$X = n \times p$ matrix of training points\\ 
\hspace{29pt} $Y = n \times 1$ vector of the classes in $X$\\
\hspace{29pt} $A = m \times p$ matrix of data to classify}
\KwResult{$B = m \times 1$ vector of classifications for $m$ data set $A$}
%-------------------------------Body--------------------------------------------
\Begin{
    $B \leftarrow $ empty $m \times 1$ vector\;
    \For{data point $y_{i} \in Y$}{
        $l \leftarrow $ empty $k$ length list\;
        \For{data point $x_{j} \in X$}{
            $d \leftarrow Dist(y_{i}, x_{j})$\;
            $c \leftarrow Y[j]$\;
            \If{$|l| < k$}{
                place $(c, d)$ in $l$ in descending order of $d$\;
            }
            \ElseIf{$l[k][2] < d$}{
                remove element $l[k]$\;
                place $(c, d)$ in $l$ in descending order of $d$\;
            }
        }
        $B[i] \leftarrow $ mode of $c$'s in $l$\;
    }
}
\caption{Serial KNN Method 1\label{KNN1}}
\end{algorithm}

\textbf{IS THERE A REFERNCE?}

\vspace{5 mm}
\noindent
On a single machine, this algorithm will take $O(n p m \log k)$. 
\textbf{THIS NEEDS A CITATOIN} 
We can modify this algorithm to run on a distributed network. Assume we have 
$S$ machines. If the data we wish to classify is small enough to fit locally on 
each distributed machine, we can then split our training data evenly across our 
machines (each machine contins $\frac{n}{S}$ data points), locally compute 
the K nearest neighbors on each of our machines using the above algorithm, and 
then reduce each locally computed answer to one machine where we compare the 
distances of $K * S << n$ elements per data point. This is easily implemented 
in a MapReduce framework.

\begin{algorithm}
%-------------------------------Header------------------------------------------
\DontPrintSemicolon
\KwData{$X = n \times p$ matrix of training points\\ 
\hspace{29pt} $Y = n \times 1$ vector of the classes in $X$\\
\hspace{29pt} $A = m \times p$ matrix of data to classify}
\KwResult{$B = m \times 1$ vector of classifications for $m$ data set $A$}
%-------------------------------Body--------------------------------------------
\Begin{
    Broadcast $A$ to our $S$ machines\;
    \For{$i$ in $p$}{
        Convert the $i$th data point $p_{i}$ in $X$ to the tuple ($i$, $p_{i}$)
        return ($\ceil*{\frac{m}{S}}$, $p_{i}$) 
    }
}
\caption{Distributed KNN Method 1 - Mapper\label{KNN1m}}
\end{algorithm}

\begin{algorithm}
%-------------------------------Header------------------------------------------
\DontPrintSemicolon
\KwData{$X = n \times p$ matrix of training points\\ 
\hspace{29pt} $Y = n \times 1$ vector of the classes in $X$\\
\hspace{29pt} $A = m \times p$ matrix of data to classify}
\KwResult{$B = m \times 1$ vector of classifications for $m$ data set $A$}
%-------------------------------Body--------------------------------------------
\Begin{
    \textbf{MAPPER:}\;
    Broadcast $A$ to our $S$ machines\;
    \For{$i$ in $p$}{
        Convert the $i$th data point $p_{i}$ in $X$ class $c_{i}$ and $Y$ to the tuple ($i$, ($p_{i}$, $c_{i}$)\;
        return ($\ceil*{\frac{m}{S}}$, ($p_{i}$, $c_{i}$)\;
    }
    \textbf{LOCALLY ON EACH MACHINE:}\;
    Run modified KNN Method 1 (only return list $k$ for each point) for each data group with key value = $i$ with $A$.\;
    


}
\caption{Distributed KNN Method 1 - Reducer\label{KNN1r}}
\end{algorithm}
