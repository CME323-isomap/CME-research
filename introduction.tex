\chapter{Introduction}

% Add citations for the description of the process?
\vspace{5 mm}
\noindent
K nearest neighbor is typically applied as a classification method. The 
intuition behind it is given some training data and a new data point, you would 
like to classify the new data based on the class of the training data that it 
is close to. Closeness is defined by a distance metric (e.g. $L_{2}$, $L_{1}$, 
etc...) applied to the feature space. We find the $k$ closest such data points 
across the whole training data and classify based on a majority class of the 
$k$ nearest training data.

\vspace{5 mm}
\noindent
The K nearest neighbor method of classification works well when similar classes 
are clustered around certain feature spaces.
\textbf{ADD CITATION} 
However, the major downside to 
implementing the K nearest neighbor method is it is computationally intense. 
In order to find our new data's $k$ closest observations in the training data, 
we will need to compare its distance to every single training data point. 
If we have training data size $N$, feature space size $P$, and assuming you 
choose a distance metric that is linear in the feature space, we need to 
perform $O(NP)$ computations to determine the a single data point's $k$ nearest 
neighbors.
\textbf{ADD CITATION}
This computation will only grow linearly with the amount of new observations 
you wish to predict.
\textbf{ADD CITATION}
With large datasets, this classification method becomes prohibitive traditional 
serial computing paradigms.
\textbf{ADD CITATION???}

\vspace{5 mm}
\noindent
K nearest neighbor also has applications beyond direct classification. It can 
be used as a way to approximate the geometric structure of data.
\textbf{ADD CITATION}
It does this by finding each data point's $k$ closest data points and 
``drawing'' lines between them. This is represented by a graph data structure, 
\textbf{ADD CITATION} but visually it can represent manifolds and geometric 
structures (see figure 1).
\textbf{ADD FIGURE SHOWING DATA AND A SHAPE} 
Performing this type of operation is used predominately in manifold learning 
techniques, such as isomaps.
\textbf{ADD CITATION}

\vspace{5 mm}
\noindent
This application of K nearest neighbors is even more computationally heavy 
because now we must compare every point in our data set to every other point. 
Again, a naive implementation would require $O(NP)$ computations per data 
point. However, we would need to do this step $O(N)$ times for each data point, 
thus ultimately requiring $O(N^{2}P)$ computations.
\textbf{ADD CITATION}

\vspace{5 mm}
\noindent
We will investigate distributed methods implementing classification with 
K nearest neighbor as well as distributing the data geometry application. The 
first has a very direct deterministic distributive method. The latter will 
require a randomized approximate approach in order to compute in a distributive 
environment.
\textbf{ADD CITATION???} 
We will approximate using a spill tree data structure.
\textbf{ADD CITATION}