\chapter{Introduction}

% Add citations for the description of the process?
\vspace{5 mm}
\noindent
K nearest neighbor is typically applied as a classification method. The 
intuition behind it is given some training data and a new data point, you would 
like to classify the new data based on the class of the training data that it 
is close to. Closeness is defined by a distance metric (e.g. $L_{2}$, $L_{1}$, 
etc...) applied to the feature space. We find the $k$ closest such data points 
across the whole training data and classify based on a majority class of the 
$k$ nearest training data.

\vspace{5 mm}
\noindent
The K nearest neighbor method of classification works well when similar classes 
are clustered around certain feature spaces [1]. However, the major downside to 
implementing the K nearest neighbor method is it is computationally intense. 
In order to find our new data's $k$ closest observations in the training data, 
we will need to compare its distance to every single training data point. 
If we have training data size $N$, feature space size $P$, and assuming you 
choose a distance metric that is linear in the feature space, we need to 
perform $O(NP)$ computations to determine the a single data point's $k$ nearest 
neighbors [2]. This computation will only grow linearly with the amount of new 
observations you wish to predict [3]. With large datasets, this classification 
method becomes prohibitively expensive for traditional serial computing 
paradigms [3].


\vspace{5 mm}
\noindent
K nearest neighbor also has applications beyond direct classification. It can 
be used as a way to approximate the geometric structure of data [2]. It does 
this by finding each data point's $k$ closest data points and ``drawing'' lines 
between them. This is represented by a graph data structure [4], but visually it 
can represent manifolds and geometric structures (see figure 1).
\textbf{ADD FIGURE SHOWING DATA AND A SHAPE} 
Performing this type of operation is used predominately in manifold learning 
techniques, such as isomaps [4].

\vspace{5 mm}
\noindent
This application of K nearest neighbors is even more computationally heavy 
because now we must compare every point in our data set to every other point. 
Again, a naive implementation would require $O(NP)$ computations per data 
point. However, we would need to do this step $O(N)$ times for each data point, 
thus ultimately requiring $O(N^{2}P)$ computations.
\textbf{ADD CITATION}

\vspace{5 mm}
\noindent
We will investigate distributed methods implementing classification with 
K nearest neighbor as well as distributing the data geometry application. The 
first has a very direct deterministic distributive method. The latter will 
require a randomized approximate approach in order to compute in a distributive 
environment.
\textbf{ADD CITATION???} 
We will approximate using a spill tree data structure.
\textbf{ADD CITATION}


\textbf{References:}
[1]: Richard Duda, Peter Hart, David Stork Duda. 
\textit{``Pattern Classification''} p.18-30, 2001.
[2]: Saravanan Thirumuruganathan. 
\textit{``A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm''}, 2010. 
[3]: Ali Dashti. 
\textit{``Efficient Computation Of K-Nearest Neighbor Graphs For Large 
High-Dimensional Data Sets On GPU Clusters''}, 2013.
[4]: Wei Dong, Moses Charikar, Kai Li. 
\textit{``EfÔ¨Åcient K-Nearest Neighbor Graph Construction for Generic 
Similarity Measures''}, 2011.
